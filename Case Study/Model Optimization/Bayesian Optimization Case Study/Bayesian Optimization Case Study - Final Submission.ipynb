{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNK7vbHo-KYU"
   },
   "source": [
    "## Bayesian methods of hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlFdvPwF-KYW"
   },
   "source": [
    "In addition to the random search and the grid search methods for selecting optimal hyperparameters, we can use Bayesian methods of probabilities to select the optimal hyperparameters for an algorithm.\n",
    "\n",
    "In this case study, we will be using the BayesianOptimization library to perform hyperparmater tuning. This library has very good documentation which you can find here: https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "You will need to install the Bayesian optimization module. Running a cell with an exclamation point in the beginning of the command will run it as a shell command — please do this to install this module from our notebook in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pssx080d-Ulf"
   },
   "outputs": [],
   "source": [
    "#! pip install bayesian-optimization\n",
    "#Running a cell with an exclamation point in the beginning of the command will run it as a shell command —\n",
    "#please do this to install this module from our notebook in the cell below\n",
    "\n",
    "#HAIN?!?! install it from YOUR notebook??? what'll that do??? why not install it ourSELVES?!?\n",
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:39:09.312682Z",
     "start_time": "2019-04-22T16:39:09.309208Z"
    },
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "l9nfFTyj-KYY"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "################################################\n",
    "from bayes_opt import BayesianOptimization\n",
    "from catboost import CatBoostClassifier, cv, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "D16Dquw1AAK0",
    "outputId": "44167587-f22e-4bf5-a816-e2bcfdc6c4ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flight_delays_test.csv.zip',\n",
       " '.DS_Store',\n",
       " 'Bayesian Optimization Case Study - Final Submission.ipynb',\n",
       " 'flight_delays_train.csv.zip',\n",
       " 'Bayesian Optimization Case Study - WORKING COPY.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'Bayesian Optimization Case Study - Original Copy.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()\n",
    "\n",
    "#'os' is operating system, checked, but why is that the thing for where these files are? this is just this folder\n",
    "#/location, aka like 'cd' or just '/' auto?\n",
    "#so this is just 'listing' everything that's in the/my (current) 'directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:48:15.929012Z",
     "start_time": "2019-04-22T14:48:15.926574Z"
    },
    "colab_type": "text",
    "id": "AkBt3yds-KYu"
   },
   "source": [
    "## How does Bayesian optimization work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1kyBCUs-KYv"
   },
   "source": [
    "Bayesian optimization works by constructing a posterior distribution of functions (Gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAdHF72R-KYw"
   },
   "source": [
    "<img src=\"https://github.com/fmfn/BayesianOptimization/blob/master/examples/bo_example.png?raw=true\" />\n",
    "As you iterate over and over, the algorithm balances its needs of exploration and exploitation while taking into account what it knows about the target function. At each step, a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with an exploration strategy (such as UCB — aka Upper Confidence Bound), or EI (Expected Improvement). This process is used to determine the next point that should be explored (see the gif below).\n",
    "<img src=\"https://github.com/fmfn/BayesianOptimization/raw/master/examples/bayesian_optimization.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTP8KUlLoYzu"
   },
   "source": [
    "## Let's look at a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crpPqKdC-KYx"
   },
   "source": [
    "The first step is to create an optimizer. It uses two items:\n",
    "* function to optimize\n",
    "* bounds of parameters\n",
    "\n",
    "The function is the procedure that counts metrics of our model quality. The important thing is that our optimization will maximize the value on function. Smaller metrics are best. Hint: don't forget to use negative metric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e09ciF8gpTfr"
   },
   "source": [
    "Here we define our simple function we want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofwvnfEwo5mG"
   },
   "outputs": [],
   "source": [
    "def simple_func(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCGsdciCpeI3"
   },
   "source": [
    "Now, we define our bounds of the parameters to optimize, within the Bayesian optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jLYW2qnpOFr"
   },
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(simple_func,{'a': (1, 3),'b': (4, 7)})\n",
    "#so it goes, BO(func,bounds_dict)\n",
    "#REPRESENTING THE ONE-TWO-STEPS! ;P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg6LdYx8pq2T"
   },
   "source": [
    "These are the main parameters of this function:\n",
    "\n",
    "* **n_iter:** This is how many steps of Bayesian optimization you want to perform. The more steps, the more likely you are to find a good maximum.\n",
    "\n",
    "* **init_points:** This is how many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmmm okay don't remember if we played w/ these 2 settings before?\n",
    "#but this does sound like the stuff we know about - like init_points must be the number of 'pre'/exploration/\n",
    "#CALIBRATION steps!!!!!!?\n",
    "#i remember we saw that, but don't know if we explicitly programmed or accepted default\n",
    "#and then n_iter is after that, once you hone in on the areas to concentrate on/cluster(s/range) of highest density,\n",
    "#and that i think i remember we did specify like 10,000? or that was the default again\n",
    "#have to go back n check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-GKMJ1uqMYv"
   },
   "source": [
    "Let's run an example where we use the optimizer to find the best values to maximize the target value for a and b given the inputs of 3 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Oy44Ro7wqNat",
    "outputId": "9cc64d54-b1e6-46d1-dc29-4c0039a1c72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     a     |     b     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m8.467    \u001b[0m | \u001b[0m2.727    \u001b[0m | \u001b[0m5.741    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m5.952    \u001b[0m | \u001b[0m1.934    \u001b[0m | \u001b[0m4.018    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m9.42     \u001b[0m | \u001b[95m2.69     \u001b[0m | \u001b[95m6.73     \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m8.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m7.0      \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m3.0      \u001b[0m | \u001b[95m7.0      \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(3,2)\n",
    "#okay so \"find the best values to maximize the target value for a & b given the inputs of 3 & 2\"...\n",
    "\n",
    "#okay so if you look at the docx, the 3,2 in order should be\n",
    "#init_points & n_iter...\n",
    "#so 3 initial calibration steps and 2 actual / production lol / 'forreal' steps\n",
    "#OHHHHHHH >>> SO IS THAT WHY THERE'S 5 TOTAL STEPS HERE?!?!?\n",
    "#but i didn't think they included/combined the calibration & production ones\n",
    "#together?!?! but also - why does it need ANY calibration\n",
    "#if you can literally just go straight to the max values of each??\n",
    "#and what is it even learning from the calibration lol\n",
    "#if so few points?? oh, well, remem it's a RANGE!\n",
    "#so can be ANY INFINITE NUMBER OF DECIMALS BETWEEN!\n",
    "#but like, if you look at it - like how did it even\n",
    "#conclude on what range to focus on / flesh out\n",
    "#in calibration?! like how could it do that in just 2 steps?\n",
    "#spreads out? and how did it suddenly figure out to try\n",
    "#exactly the max values lol?\n",
    "####################################################\n",
    "#NICE!!! so aH we were right about/confirmed what 3,2 are by manually specifying, below\n",
    "#AND ALSO! our theory at least lines up w/ later example here of optimizer/maximizer\n",
    "#in that that's 10,2 and has a total of *12* rows!!!!!!\n",
    "########################################################\n",
    "\n",
    "#ok so remem we specified the range of a & b above! 1->3 & 4->7, resp\n",
    "#note, 3,2 aren't a,b! this isn't the FUNCTION we made! this is a method call! we ALREADY\n",
    "#specified a,b - that WENT *INTO* THE OPTIMIZER!!!!\n",
    "#plus, wouldn't make sense anyway cuz 2 is out of range for b....\n",
    "\n",
    "#okay, so, if we look below, the winner is 'target = 10, from a=3 + b=7',....\n",
    "#okay so - that's LITERALLY simply maximizing the function given the ranges, cuz 3 & 7 are RANGE TOPPERS resp!!!!\n",
    "########################################################\n",
    "#like it's saying find the values that maximize the \"target value\"\n",
    "#THE 'TARGET VALUE' SIMPLY REFERS TO THE THING WE'RE TRYNA PREDICT, AKA THE *Y*!!!!!!\n",
    "#AKA 'TARGET *FEATURE*!!!!! like here it's y = a + b!!!!\n",
    "#SO YEAH IF a GOES UP TO MAX 3 & Y GOES UP TO MAX 7, THEN MAX(A+B)=MAX(A)+MAX(B)=7+3=10!!!\n",
    "#so like if we plotted this line, it'd literally be a small little line segment noodle\n",
    "#in 3D SPACE!!!!!!\n",
    "#'maximizing' the target value literally means just making it the maximum possible lolll\n",
    "#i was confused first at the terminology 'target value' cuz i'm thinking that means like\n",
    "#'there's a certain literally *TARGET* value we're tryna reach!!!!\n",
    "\n",
    "#obvy in this case we're not tryna \"predict\" y (right?) but rather it's just our f(x)\n",
    "#just like how we had/used the function for the Normal Distribution Equation\n",
    "#there, we were tryna choose the best hyperparameter value, which we used as the x\n",
    "#in that equation, and since we had 2 hyper/params, we had to use that equation for EACH!\n",
    "#but it's interesting!!!!! those two hyperparams were for their OWN equation where\n",
    "#they both coexisted, aka *THE BEST FIT SLOPE LINE Y=MX+B!!!!!!\n",
    "#but rather than solving m & b analytically like using the equation for OLS,\n",
    "#we do it like here BAYESIAN PROBABILISTICALLY/DISTRIBUTIONALISTICALLY!!!!!!\n",
    "#so yeah DON'T GET CONFUSED BY THE EQUATION *OF* THE HYPERPARAMS WE'RE TRYNA OPTIMIZE\n",
    "#&THE EQUATION WE'RE *USING* TO OPTIMIZE THE HYPERPARAMS!!!!!!!!!!!!\n",
    "#*****BUT NOTE***** in this VERY VERY simplified sample example, we made things super\n",
    "#easy - a & b first of all ARE our hyperparameters, and so the SAME equation y=a+b is\n",
    "#both the equation OF our hyperparameters and the equation TO/FOR *MAXIMIZE* OUR\n",
    "#HYPERPARAMETERS!!!!!!*******************\n",
    "###############################################################################################\n",
    "#NOTE! in y=mx+b Bayesian we were predicting in the sense of that is what we’re AIMING to do with the y=mx+b equation ultimately!!!!! and like when we’re optimizing, we’re picking / testing m/b’s to come up with PREDICTED Y’S!!!!!! that we use for our yp’s in the NormDistEqn!!!!!\n",
    "#In this case study as we’ll see, we ARE gonna use Bayes for prediction but it’ll be CLASSIFICATION and will look very different than what we saw w/ NUTS! you’ll see about that below!\n",
    "\n",
    "#so yeah here in this simple example - we're not tryna predict! the y of the function we choose to optimize\n",
    "#our parameters simply tells us what parameter value to choose since we're MAXIMIZING\n",
    "#the function aka literally getting the MAX F(X) AKA *MAX Y* and using the X-value\n",
    "#aka HYPERPARAM VALUE THAT GIVES US THAT MAXIMUM!!!!!!\n",
    "#in Normal Dist Equation example, that's the optimizer function and so the units\n",
    "#are *Probability Density* aka that's our METRIC!!!! but it REPRESENTS/IS *INVERSELY* PROPORTIONAL TO (right?)\n",
    "#SQUARED ERROR!!!!!! so - really we're doing the same thing in OLS as we are in Bayesian - the objective is\n",
    "#the same, we're tryna MINIMIZE THE SQUARED ERROR!\n",
    "#it's just that in OLS, we're doing it EXACTLY, aka \"analytically\", w/ an equation for the slope\n",
    "#of the best fit line (the (IN)famous/notorious one we well know!) - that works out the be the slope of the line\n",
    "#that minimizes the squared error/distance to the points overall\n",
    "#then of course we can just plug & chug into y=mx+b, having m and using the CENTER x,y as the x,y\n",
    "#to put in here, since we KNOW the line will run thru the CENTER point, and thus can solve/get b!\n",
    "#but NOTE in OLS we're not doing it 'DIRECTLY', as i previously thought of it as, cuz if you think about it,\n",
    "#that would mean we're LITERALLY using an equation for the squared error of all the proposed predicted points,\n",
    "#but that's NOT what we're doing, so you could say actually that OLS is *INDIRECTLY* minimizing the error!!!!!\n",
    "#cuz again we're just using the actuals ALONE! aka JUST the actual X,y's and the center/their centers, basically\n",
    "#\"spreading\" the \"area to center\"!!!!!!! and what results *IS* the slope/intercept/line that minimizes the\n",
    "#squared error to the points!!!!!!\n",
    "#Now, w/ Bayesian probability estimation, we actually *ARE* focused DIRECTLY on squared error,\n",
    "#w/ just (yp-yi)^2, where we can't actually SOLVE it for the minimum, but rather, can just do *\"TRIAL & ERROR\"*\n",
    "#testing out diff lines aka diff m/b aka SLOPE/INTERCEPT COMBOS for diff y=mx+b equations to GIVE US\n",
    "#DIFFERENT SETS OF PREDICTED POINTS AND FOR EACH SET FOR EACH M/B AKA Y=MX+B COMBO/STRAIGHT EDGE/LINE/EQUATION\n",
    "#WE CALCULATE THE SQUARED ERROR AND THEORETICALLY THERE'S *INFINITE* COMBOS TO TRY SO WE'RE JUST TRYNA *NARROW\n",
    "#IT DOWN*!!!!!! HONE IN TILL WE REALISTICALLY GET THE EFFECTIVE OPTIMAL VALUE PAIR/COMBO OF M/B!//SLOPE/INTERCEPT!!!!!!\n",
    "#but the thing is, we don't use that equation/measure *DIRECTLY* lol so this is also INDIRECT in this sense,\n",
    "#that we use the NORMAL DISTRIBUTION EQUATION because we can hack it to use it for SQUARED DIFFERENCE also\n",
    "#since that's apart of the equation, but also, this \"transforms\" the values too in our favor where the\n",
    "#smaller the squared error, the bigger the f(x)/value of the equation, which is perfect since we wanna find\n",
    "#the MAX of f(x)//y, thus, the *MINIMUM SQUARED ERROR IS THE MAXIMUM Y//F(X)!!!!!!!!!!!!!!!!!!!!!!!!!!!*\n",
    "#and i forget the details but we do do this separately each for slope & intercept? so even though we need BOTH\n",
    "#in order to come up with an equation/line to get our y's...., but yeah like there's a certain range of i guess\n",
    "#optimal / best range / target m's/slopes and same for intercepts and so we're looking for the combo that\n",
    "#TOGETHER gives us maximum of helper function aka MINIMUM SQUARED ERROR!!!!!\n",
    "#or you know what? it might actually just be TWO DIFFERENT *VIEWS* OF THE *SAME THING*!!!!!!!!\n",
    "#just like if you have more than 2 dimensions but are only PLOTTING ON 2 dimensions, you can be looking at the\n",
    "#SAME THING, just from different angles - cuz yeah like you DO need both slope & intercept to come up w/ the\n",
    "#y's to try out to calculate that squared error/f(x) - so i think it's like you're really doing ONE simulation,\n",
    "#where you're testing out BOTH values simultaneously cuz they're a COMBO aka **IN TANDEM**, so yeah they're both\n",
    "#showing the same thing, just different angles >> but wait... they are doing independent calculations tho aren't\n",
    "#they? so then how are they different? if they both needa test both? are they like each attacking it from a\n",
    "#different angle and just simply ARRIVING AT the SAME CONCLUSION/DESTINATION?!?!??!?!!?!?\n",
    "#but like i don't know how you really have different approaches/focuses if you need both cuz like even if one\n",
    "#is 'focusing on the intercept' let's say... well it still needs to go thru the range of intercepts and pair\n",
    "#it w/ the range of slopes. i mean yeah i guess you could literally just do the ORDER backwards of like, you go\n",
    "#thru one intercept at a time, and try all slopes for it, while slope-focused one goes thru each slope,\n",
    "#trying each intercept in the range w/ it, but then that kinda seems like a waste???/redundant\n",
    "#unless they were both tryna like each independently focus on figuring out the optimum range for its piece/param,\n",
    "#but that doesn't seem to be the case - again, both use both, and plus, seems like that's what CALIBRATION PERIOD\n",
    "#IS FOR!!!!!!\n",
    "\n",
    "#but yeah so, keep in mind that it's not like OLS where you can JUST find slope\n",
    "#and then cuz we can just solve intercept from y=mx+b then - doesn't work like that here cuz we CAN'T\n",
    "#SOLVE SLOPE & INTERCEPT INDEPENDENTLY!!!!!! WE SOLVE THEM *SIMULTANEOUSLY* THRU *TRIAL&ERROR*!!!!!!!!!!\n",
    "#gotta guess at both to make combos!!!!!!\n",
    "#but again, don't get confused - even tho we're running TWO equations in Bayes, it's not like each one is\n",
    "#JUST FOR IT/THAT PARAM!!!!!!!!!!!\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "#so yeah, HERE IN BAYESIAN PROBABILITY/ESTIMATION WE'RE *STILL* MINIMIZING SQUARED ERROR!!!!!!! BUT USING\n",
    "#A DIFFERENT EQUATION AND DOING SO *PROBABILISTICALLY*!!!!!!! lol \"ballistically\" ;P\n",
    "#so because of the nature of the equation, it allows us to have this perfect inverse\n",
    "#relationship that works out for us to give us the MAXIMUM OF OUR HELPER/HYPERPARAM\n",
    "#OPTIMIZER FUNCTION @ THE *MINIMUM* OF OUR METRIC OF SQUARED ERROR!!!!!!\n",
    "#or actually, is the 'metric' considered the/this y of the optimizer function???\n",
    "###############################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     a     |     b     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m6.146    \u001b[0m | \u001b[0m1.113    \u001b[0m | \u001b[0m5.033    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m7.668    \u001b[0m | \u001b[95m1.643    \u001b[0m | \u001b[95m6.025    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m8.089    \u001b[0m | \u001b[95m1.974    \u001b[0m | \u001b[95m6.114    \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m9.386    \u001b[0m | \u001b[95m2.386    \u001b[0m | \u001b[95m7.0      \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m3.0      \u001b[0m | \u001b[95m7.0      \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(n_iter=3,init_points=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTICE YOU'LL GET DIFF RESULTS EVERY TIME!!!! is it cuz of RANDOMNESS?? no random_state /_seed option??\n",
    "#as far as the SPECIFICS, aka the JOURNEY to getting to the max,\n",
    "#but it'll still settle on/converge on/ARRIVE AT THE *SAME MAX* OF THE TARGET VALUE!!!!!!!\n",
    "#their results are yet ANOTHER variation! but again, same max result/conclusion/solution!\n",
    "#A ROW GETS HIGHLIGHTED PINK WHEN/IF IT'S THE REIGNING/INCUMBENT MAX (minus/except for the 1st),\n",
    "#aka KING'S COURT RULES!!!!!Ruler lol - Reign until you get dethroned! but this shows the progression/journey!\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyKFMF2Hq2Sx"
   },
   "source": [
    "Great, now let's print the best parameters and the associated maximized target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_H6DixyfscV_",
    "outputId": "fd0c35d7-e30d-4d30-9ab2-12c0fa837971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 3.0, 'b': 7.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(optimizer.max['params']);optimizer.max['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(if remove 'print()')\n",
    "optimizer.max['params'];optimizer.max['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQ1T1V6Mspi4"
   },
   "source": [
    "## Test it on real data using the Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_oGwREZkm4h"
   },
   "source": [
    "The dataset we will be working with is the famous flight departures dataset. Our modeling goal will be to predict if a flight departure is going to be delayed by 15 minutes based on the other attributes in our dataset. As part of this modeling exercise, we will use Bayesian hyperparameter optimization to identify the best parameters for our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abYSagjQANDZ"
   },
   "source": [
    "**<font color='teal'> You can load the zipped csv files just as you would regular csv files using Pandas read_csv. In the next cell load the train and test data into two separate dataframes. </font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWKBApVuAeJe"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('flight_delays_train.csv.zip')\n",
    "test_df = pd.read_csv('flight_delays_test.csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OapNcT9Eikis"
   },
   "source": [
    "**<font color='teal'> Print the top five rows of the train dataframe and review the columns in the data. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "__4cXZ8iiYaC",
    "outputId": "8718ad4b-8955-486c-9ae8-1dee6aa6c2fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>dep_delayed_15min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c-8</td>\n",
       "      <td>c-21</td>\n",
       "      <td>c-7</td>\n",
       "      <td>1934</td>\n",
       "      <td>AA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>732</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c-4</td>\n",
       "      <td>c-20</td>\n",
       "      <td>c-3</td>\n",
       "      <td>1548</td>\n",
       "      <td>US</td>\n",
       "      <td>PIT</td>\n",
       "      <td>MCO</td>\n",
       "      <td>834</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c-9</td>\n",
       "      <td>c-2</td>\n",
       "      <td>c-5</td>\n",
       "      <td>1422</td>\n",
       "      <td>XE</td>\n",
       "      <td>RDU</td>\n",
       "      <td>CLE</td>\n",
       "      <td>416</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c-11</td>\n",
       "      <td>c-25</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1015</td>\n",
       "      <td>OO</td>\n",
       "      <td>DEN</td>\n",
       "      <td>MEM</td>\n",
       "      <td>872</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c-10</td>\n",
       "      <td>c-7</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1828</td>\n",
       "      <td>WN</td>\n",
       "      <td>MDW</td>\n",
       "      <td>OMA</td>\n",
       "      <td>423</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Month DayofMonth DayOfWeek  DepTime UniqueCarrier Origin Dest  Distance  \\\n",
       "0   c-8       c-21       c-7     1934            AA    ATL  DFW       732   \n",
       "1   c-4       c-20       c-3     1548            US    PIT  MCO       834   \n",
       "2   c-9        c-2       c-5     1422            XE    RDU  CLE       416   \n",
       "3  c-11       c-25       c-6     1015            OO    DEN  MEM       872   \n",
       "4  c-10        c-7       c-6     1828            WN    MDW  OMA       423   \n",
       "\n",
       "  dep_delayed_15min  \n",
       "0                 N  \n",
       "1                 N  \n",
       "2                 N  \n",
       "3                 N  \n",
       "4                 Y  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so based on month, day of month, day of WEEK, time of day, carrier, origin, destination & distance\n",
    "#then predictor/target feature is BINARY yes/no for whether delayed 15 mins or not (AT LEAST 15 mins, obv,\n",
    "#i'm assuming). So note, basically the definition for 'delay' is that it was AT LEAST 15 minutes! if less than that,\n",
    "#then grace period lol, off the hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxGBsPQhffgd"
   },
   "source": [
    "**<font color='teal'> Use the describe function to review the numeric columns in the train dataframe. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "_bRRKG3DAtae",
    "outputId": "7cfb9975-ec97-422c-abbd-98923a0b7aec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepTime</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1341.523880</td>\n",
       "      <td>729.39716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>476.378445</td>\n",
       "      <td>574.61686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>931.000000</td>\n",
       "      <td>317.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1330.000000</td>\n",
       "      <td>575.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1733.000000</td>\n",
       "      <td>957.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2534.000000</td>\n",
       "      <td>4962.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DepTime      Distance\n",
       "count  100000.000000  100000.00000\n",
       "mean     1341.523880     729.39716\n",
       "std       476.378445     574.61686\n",
       "min         1.000000      30.00000\n",
       "25%       931.000000     317.00000\n",
       "50%      1330.000000     575.00000\n",
       "75%      1733.000000     957.00000\n",
       "max      2534.000000    4962.00000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepTime</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1341.5</td>\n",
       "      <td>729.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>476.4</td>\n",
       "      <td>574.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>931.0</td>\n",
       "      <td>317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1330.0</td>\n",
       "      <td>575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1733.0</td>\n",
       "      <td>957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2534.0</td>\n",
       "      <td>4962.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DepTime  Distance\n",
       "count  100000.0  100000.0\n",
       "mean     1341.5     729.4\n",
       "std       476.4     574.6\n",
       "min         1.0      30.0\n",
       "25%       931.0     317.0\n",
       "50%      1330.0     575.0\n",
       "75%      1733.0     957.0\n",
       "max      2534.0    4962.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>dep_delayed_15min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c-8</td>\n",
       "      <td>c-21</td>\n",
       "      <td>c-7</td>\n",
       "      <td>1934</td>\n",
       "      <td>AA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>732</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c-4</td>\n",
       "      <td>c-20</td>\n",
       "      <td>c-3</td>\n",
       "      <td>1548</td>\n",
       "      <td>US</td>\n",
       "      <td>PIT</td>\n",
       "      <td>MCO</td>\n",
       "      <td>834</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c-9</td>\n",
       "      <td>c-2</td>\n",
       "      <td>c-5</td>\n",
       "      <td>1422</td>\n",
       "      <td>XE</td>\n",
       "      <td>RDU</td>\n",
       "      <td>CLE</td>\n",
       "      <td>416</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c-11</td>\n",
       "      <td>c-25</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1015</td>\n",
       "      <td>OO</td>\n",
       "      <td>DEN</td>\n",
       "      <td>MEM</td>\n",
       "      <td>872</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c-10</td>\n",
       "      <td>c-7</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1828</td>\n",
       "      <td>WN</td>\n",
       "      <td>MDW</td>\n",
       "      <td>OMA</td>\n",
       "      <td>423</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>c-5</td>\n",
       "      <td>c-4</td>\n",
       "      <td>c-3</td>\n",
       "      <td>1618</td>\n",
       "      <td>OO</td>\n",
       "      <td>SFO</td>\n",
       "      <td>RDD</td>\n",
       "      <td>199</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>c-1</td>\n",
       "      <td>c-18</td>\n",
       "      <td>c-3</td>\n",
       "      <td>804</td>\n",
       "      <td>CO</td>\n",
       "      <td>EWR</td>\n",
       "      <td>DAB</td>\n",
       "      <td>884</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>c-1</td>\n",
       "      <td>c-24</td>\n",
       "      <td>c-2</td>\n",
       "      <td>1901</td>\n",
       "      <td>NW</td>\n",
       "      <td>DTW</td>\n",
       "      <td>IAH</td>\n",
       "      <td>1076</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>c-4</td>\n",
       "      <td>c-27</td>\n",
       "      <td>c-4</td>\n",
       "      <td>1515</td>\n",
       "      <td>MQ</td>\n",
       "      <td>DFW</td>\n",
       "      <td>GGG</td>\n",
       "      <td>140</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>c-11</td>\n",
       "      <td>c-17</td>\n",
       "      <td>c-4</td>\n",
       "      <td>1800</td>\n",
       "      <td>WN</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SMF</td>\n",
       "      <td>605</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Month DayofMonth DayOfWeek  DepTime UniqueCarrier Origin Dest  Distance  \\\n",
       "0       c-8       c-21       c-7     1934            AA    ATL  DFW       732   \n",
       "1       c-4       c-20       c-3     1548            US    PIT  MCO       834   \n",
       "2       c-9        c-2       c-5     1422            XE    RDU  CLE       416   \n",
       "3      c-11       c-25       c-6     1015            OO    DEN  MEM       872   \n",
       "4      c-10        c-7       c-6     1828            WN    MDW  OMA       423   \n",
       "...     ...        ...       ...      ...           ...    ...  ...       ...   \n",
       "99995   c-5        c-4       c-3     1618            OO    SFO  RDD       199   \n",
       "99996   c-1       c-18       c-3      804            CO    EWR  DAB       884   \n",
       "99997   c-1       c-24       c-2     1901            NW    DTW  IAH      1076   \n",
       "99998   c-4       c-27       c-4     1515            MQ    DFW  GGG       140   \n",
       "99999  c-11       c-17       c-4     1800            WN    SEA  SMF       605   \n",
       "\n",
       "      dep_delayed_15min  \n",
       "0                     N  \n",
       "1                     N  \n",
       "2                     N  \n",
       "3                     N  \n",
       "4                     Y  \n",
       "...                 ...  \n",
       "99995                 N  \n",
       "99996                 N  \n",
       "99997                 N  \n",
       "99998                 N  \n",
       "99999                 N  \n",
       "\n",
       "[100000 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100,THOUSAND ENTRIES!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohhh right, so automatically ONLY NUMERICAL FEATURES ARE INCLUDED!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   Month              100000 non-null  object\n",
      " 1   DayofMonth         100000 non-null  object\n",
      " 2   DayOfWeek          100000 non-null  object\n",
      " 3   DepTime            100000 non-null  int64 \n",
      " 4   UniqueCarrier      100000 non-null  object\n",
      " 5   Origin             100000 non-null  object\n",
      " 6   Dest               100000 non-null  object\n",
      " 7   Distance           100000 non-null  int64 \n",
      " 8   dep_delayed_15min  100000 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6k-_fI5Aiyh"
   },
   "source": [
    "Notice, `DepTime` is the departure time in a numeric representation in 2400 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmm, is it appropriate for us to use TIME tho, since it's CIRCULAR? that's interesting right,\n",
    "#almost like its OWN SEPARATE CATEGORY! from discrete & continuous (and not 'discrete style continuous either lol)\n",
    "#cuz like 2359 (11:59PM) is basically the SAME as 0000 (12AM MIDNIGHT!)... but they're MAXIMALLY SEPARATED\n",
    "#ON THIS SCALE!!!!!!!! SO HOW DOES THAT WORK?!?!\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtZS4-hrlQah"
   },
   "source": [
    " **<font color='teal'>The response variable is 'dep_delayed_15min' which is a categorical column, so we need to map the Y for yes and N for no values to 1 and 0. Run the code in the next cell to do this.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   Month              100000 non-null  object\n",
      " 1   DayofMonth         100000 non-null  object\n",
      " 2   DayOfWeek          100000 non-null  object\n",
      " 3   DepTime            100000 non-null  int64 \n",
      " 4   UniqueCarrier      100000 non-null  object\n",
      " 5   Origin             100000 non-null  object\n",
      " 6   Dest               100000 non-null  object\n",
      " 7   Distance           100000 non-null  int64 \n",
      " 8   dep_delayed_15min  100000 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nice no at least APPARENT/LITERAL BLANKS!!!!! doesn't guarantee no MISSINGS or BALONEYS/BOGEYS!!!!!/BOGUESS! :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Y'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dep_delayed_15min.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:38:42.677690Z",
     "start_time": "2019-04-22T15:38:42.481963Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yRlOTbnW-KYc"
   },
   "outputs": [],
   "source": [
    "#train_df = train_df[train_df.DepTime <= 2400].copy()\n",
    "#NOTE^^This was just here, already commented out, they don't say anything about this, but is this\n",
    "#to filter bogus values? OH! yeah if we look @/to the describe table, we see the max time *IS* >2400!!!!!\n",
    "#SO THERE MAY BE OTHERS!!!!\n",
    "#BUT THEN WHY WOULDN'T THEY RUN THIS?!?! i guess maybe they're simplifying it for us for now but reminding us of\n",
    "# / giving us an idea of some kind of stuff/examples for preprocessing / cleanup we should be looking for normally\n",
    "#on the job / in the field.... Lol or maybe commenting it out was legit an accident :P\n",
    "#########################################################################################################\n",
    "y_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\n",
    "#note gotta convert to NUMBERS ofc!\n",
    "#SO REMEM! .MAP() IS A SERIOUS TOOL! YOU HAVE TO USE IT VERY CAREFULLY/BE VERY SURE!\n",
    "#compared to .replace(manual_mapping_dict), like we usually do, .map() will RENDER ANYTHING THAT'S *NOT*\n",
    "#A MATCH AKA *NOT* CONTAINED IN THE DICT AS A *NAN*!!!!!!!!!\n",
    "#but we can check w/ a .unique, esp w/ binaries it's very easy - as done above!^\n",
    "#and maybe it's that the only non-specified values ARE nans, so those won't be affected lol, but we shoulda\n",
    "#started w/ confirming there aren't any of those/removing any if there are!\n",
    "#checked above^ and we're good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohhhh okay, sA, just realized this is different than what we usually do cuz\n",
    "#we're working on / treating / cleaning & doing EDA *ONLY* on the TRAINING\n",
    "#portion/split?!?!?! WHY'S THAT?!? why didn't we START with the FULL??\n",
    "#is this like SUPER 'data leakage' / cybersecurity paranoia?! lol\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3WPkFQO9uo9"
   },
   "source": [
    "## Feature Engineering\n",
    "Use these defined functions to create additional features for the model. Run the cell to add the functions to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXqsqz5W9t3r"
   },
   "outputs": [],
   "source": [
    "#DON'T KNOW! but gives us mapping we use later\n",
    "def label_enc(df_column): #hain?\n",
    "    df_column = LabelEncoder().fit_transform(df_column)\n",
    "    return df_column\n",
    "\n",
    "def make_harmonic_features_sin(value, period=2400): #hain?\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.sin(value)\n",
    "\n",
    "def make_harmonic_features_cos(value, period=2400): #hain?\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.cos(value)\n",
    "\n",
    "#'MANUFACTURED FEATURES' AKA *FEATURE ENGINEERING* INDEED!!!!!\n",
    "def feature_eng(df):\n",
    "    #THESE GIVE US *ADDITIONAL GROUPINGS* FOR FLIGHTS SO THAT WE CAN SEE WHAT OTHER FLIGHTS WERE THAT SAME COMBO!\n",
    "    df['flight'] = df['Origin']+df['Dest'] #i guess if there are REPEAT routes in here then we can see if there are any patterns/consistencies w/ those! note that ORDER MATTERS!!!! like CHI-SEA is diff than SEA-CHI!!!!! MAKES SENSE THO!!!!!\n",
    "    df['flightUC'] = df['flight']+df['UniqueCarrier'] #this one builds off what we just made!\n",
    "    df['DestUC'] = df['Dest']+df['UniqueCarrier'] #so like MDW-SOUTHWEST\n",
    "    df['OriginUC'] = df['Origin']+df['UniqueCarrier'] #also could be MDW-SOUTHWEST! but since these are DIFF columns, they would be totally separate! like it wouldn't make sense for the origin and destination city to be the same!\n",
    "\n",
    "    #these ones just clean up the formatting, like removing the weird 'c-' prefix from these date monikers\n",
    "    df['Month'] = df.Month.map(lambda x: x.split('-')[-1]).astype('int32')\n",
    "    df['DayofMonth'] = df.DayofMonth.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['DayOfWeek'] = df.DayOfWeek.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "\n",
    "    #puts a decimal between the first two//hour numbers & second two//minute numbers of the military time stamp,\n",
    "    #i guess to make easier to read? kinda serving like purpose of a 'colon:' but RETAINS its NUMERICAL identity!\n",
    "    #which again i'm skeptical about\n",
    "    df['hour'] = df.DepTime.map(lambda x: x/100).astype('int32')\n",
    "\n",
    "    #these ones create categories / labels / groups! but rather than CUTTING which REPLACES specifics/numbers\n",
    "    #w/ broader general categories, this is in ADDITION TO... But i guess we could achieve the same thing\n",
    "    #w/ cutting if we just created a COPIED COLUMN?!?!\n",
    "    ########################################################################\n",
    "    #labels for time of month\n",
    "    df['begin_of_month'] = (df['DayofMonth'] < 10).astype('uint8')\n",
    "    df['midddle_of_month'] = ((df['DayofMonth'] >= 10)&(df['DayofMonth'] < 20)).astype('uint8')\n",
    "    df['end_of_month'] = (df['DayofMonth'] >= 20).astype('uint8')\n",
    "    #labels for time of day\n",
    "    df['morning'] = df['hour'].map(lambda x: 1 if (x <= 11)& (x >= 7) else 0).astype('uint8')\n",
    "    df['day'] = df['hour'].map(lambda x: 1 if (x >= 12) & (x <= 18) else 0).astype('uint8')\n",
    "    df['evening'] = df['hour'].map(lambda x: 1 if (x >= 19) & (x <= 23) else 0).astype('uint8')\n",
    "    df['night'] = df['hour'].map(lambda x: 1 if (x >= 0) & (x <= 6) else 0).astype('int32')\n",
    "    #labels for time of YEAR!\n",
    "    df['winter'] = df['Month'].map(lambda x: x in [12, 1, 2]).astype('int32')\n",
    "    df['spring'] = df['Month'].map(lambda x: x in [3, 4, 5]).astype('int32')\n",
    "    df['summer'] = df['Month'].map(lambda x: x in [6, 7, 8]).astype('int32')\n",
    "    df['autumn'] = df['Month'].map(lambda x: x in [9, 10, 11]).astype('int32')\n",
    "\n",
    "    #labels for Weekend vs. Weekday... though, it says 'Holiday'... but that's misleading, though a GOOD\n",
    "    #idea to have a THIRD if possible for ACTUAL like federal US holidays!!!\n",
    "    ########################################################################\n",
    "    df['holiday'] = (df['DayOfWeek'] >= 5).astype(int) \n",
    "    df['weekday'] = (df['DayOfWeek'] < 5).astype(int)\n",
    "\n",
    "    #These are like counts/metrics pertaining to one piece of that row/record\n",
    "    #wait these all do *.TRANSFORM* - isn't that only for MODELS that have been FIT?!\n",
    "    ########################################################################\n",
    "    #we can make sense of everything OUTSIDE of the .transform(), aka the GROUPBY, but don't understand the .transform()...\n",
    "    #so i'll at least comment on the groupbys counts\n",
    "    df['airport_dest_per_month'] = df.groupby(['Dest', 'Month'])['Dest'].transform('count') #dest airport & month count\n",
    "    df['airport_origin_per_month'] = df.groupby(['Origin', 'Month'])['Origin'].transform('count') #orig airport & month count\n",
    "    df['airport_dest_count'] = df.groupby(['Dest'])['Dest'].transform('count') #just dest airport count\n",
    "    df['airport_origin_count'] = df.groupby(['Origin'])['Origin'].transform('count') #TOTAL number of origins... couldn't we just do like len(df.col.unique())? or df.col.unique().count()??\n",
    "    df['carrier_count'] = df.groupby(['UniqueCarrier'])['Dest'].transform('count') #TOTAL number of destinations each carrier/airline had\n",
    "    df['carrier_count_per month'] = df.groupby(['UniqueCarrier', 'Month'])['Dest'].transform('count') #number of destinations that airline had each month\n",
    "    \n",
    "    #OHHHHHHHHHHHHHHHHHHHHHHHHHHHH!\n",
    "    #okay now aH i think i get it for what .transform is doing\n",
    "    #so, this is something i've done in my own excel analytics gymnastics acrobatics lol\n",
    "    #it's basically putting some summary value for some grouping related to that row, for *EVERY* grouping in that row!!!\n",
    "    #so, we start off w/ each of these as making a new column right\n",
    "    #so the purpose of that column is to have the value of the grouping that pertains to THAT ROW!!!!\n",
    "    #so EVERY member/row belonging to a grouping has the *SAME* exact value for that column!!!!!!!!!!\n",
    "    #so like, let's pick a simple one - airport_origin_count - so that's ONLY grouped on every ORIGIN,\n",
    "    #thus, remem w/ each row being a *FLIGHT*, ALL the flights of the SAME origin will show the SAME NUMBER/VALUE\n",
    "    #FOR THAT COLUMN OF THE TOTAL NUMBER OF (SUM) FLIGHTS WHERE THAT ORIGIN AIRPORT WAS THE ORIGIN AIRPORT!\n",
    "    #SO AKA HOW MANY FLIGHTS IN THIS DATAFRAME ARE FROM THAT ORIGIN!!!!!!\n",
    "    #SO SAY/TAKE CHICAGO MIDWAY (MDW) - SAY 100 OF THE FLIGHTS IN THIS DATAFRAME ARE MIDWAY ORIGIN FLIGHTS;\n",
    "    #THEN, FOR THIS 'AIRPORT_ORIGIN_COUNT' FIELD, FOR *EVERY* MIDWAY FLIGHT, *IT'S GONNA SAY/BE *100*!!!!!!\n",
    "    ####################################################################################################\n",
    "\n",
    "    \n",
    "    #STILL FIGURING WHAT THIS IS ALL ABOUT\n",
    "    df['deptime_cos'] = df['DepTime'].map(make_harmonic_features_cos)\n",
    "    df['deptime_sin'] = df['DepTime'].map(make_harmonic_features_sin)\n",
    "\n",
    "\n",
    "    return df.drop('DepTime', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniqueCarrier  Month\n",
       "AA             c-1      797\n",
       "               c-10     770\n",
       "               c-11     756\n",
       "               c-12     785\n",
       "               c-2      708\n",
       "                       ... \n",
       "YV             c-5      170\n",
       "               c-6      190\n",
       "               c-7      198\n",
       "               c-8      194\n",
       "               c-9      179\n",
       "Name: Dest, Length: 261, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['UniqueCarrier', 'Month'])['Dest'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dest  Month\n",
       "ABE   c-1      7\n",
       "      c-10     3\n",
       "      c-11     7\n",
       "      c-12     4\n",
       "      c-2      8\n",
       "              ..\n",
       "YUM   c-5      5\n",
       "      c-6      3\n",
       "      c-7      3\n",
       "      c-8      2\n",
       "      c-9      2\n",
       "Name: Dest, Length: 3037, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#okay so if we take this as an example:\n",
    "#train_df.groupby(['Dest', 'Month'])['Dest'].transform('count')\n",
    "\n",
    "#well, let's take out the .transform for now and see\n",
    "\n",
    "#okay so this does what we expect aH - groups each destination airport & month combo\n",
    "#and simply counts how many times each combo comes up\n",
    "\n",
    "train_df.groupby(['Dest', 'Month'])['Dest'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        373\n",
       "1        168\n",
       "2        104\n",
       "3         46\n",
       "4         21\n",
       "        ... \n",
       "99995      2\n",
       "99996      4\n",
       "99997    266\n",
       "99998      2\n",
       "99999     68\n",
       "Name: Dest, Length: 100000, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#okay so then... what the heck is .transform doing?\n",
    "train_df.groupby(['Dest', 'Month'])['Dest'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#hmmm - yeah not sure... it's got one number for every ROW in the df... but what\n",
    "#would that number represent? but also, it's a transform on the GROUPBY/pivot...\n",
    "#which ISN'T all the diff original indiv rows.... So not sure how it's coming up w/ that??\n",
    "\n",
    "#>>>>>>>>>>>>>NOW FIGURED OUT! PUT ABOVE!\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BYbxXpU-FGE"
   },
   "source": [
    "Concatenate the training and testing dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lol reverse of how we usually do it... but kinda like serious segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c-7</td>\n",
       "      <td>c-25</td>\n",
       "      <td>c-3</td>\n",
       "      <td>615</td>\n",
       "      <td>YV</td>\n",
       "      <td>MRY</td>\n",
       "      <td>PHX</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c-4</td>\n",
       "      <td>c-17</td>\n",
       "      <td>c-2</td>\n",
       "      <td>739</td>\n",
       "      <td>WN</td>\n",
       "      <td>LAS</td>\n",
       "      <td>HOU</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c-12</td>\n",
       "      <td>c-2</td>\n",
       "      <td>c-7</td>\n",
       "      <td>651</td>\n",
       "      <td>MQ</td>\n",
       "      <td>GSP</td>\n",
       "      <td>ORD</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c-3</td>\n",
       "      <td>c-25</td>\n",
       "      <td>c-7</td>\n",
       "      <td>1614</td>\n",
       "      <td>WN</td>\n",
       "      <td>BWI</td>\n",
       "      <td>MHT</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c-6</td>\n",
       "      <td>c-6</td>\n",
       "      <td>c-3</td>\n",
       "      <td>1505</td>\n",
       "      <td>UA</td>\n",
       "      <td>ORD</td>\n",
       "      <td>STL</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>c-6</td>\n",
       "      <td>c-5</td>\n",
       "      <td>c-2</td>\n",
       "      <td>852</td>\n",
       "      <td>WN</td>\n",
       "      <td>CRP</td>\n",
       "      <td>HOU</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>c-11</td>\n",
       "      <td>c-24</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1446</td>\n",
       "      <td>UA</td>\n",
       "      <td>ORD</td>\n",
       "      <td>LAS</td>\n",
       "      <td>1515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>c-1</td>\n",
       "      <td>c-30</td>\n",
       "      <td>c-2</td>\n",
       "      <td>1509</td>\n",
       "      <td>OO</td>\n",
       "      <td>ORD</td>\n",
       "      <td>SGF</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>c-1</td>\n",
       "      <td>c-5</td>\n",
       "      <td>c-5</td>\n",
       "      <td>804</td>\n",
       "      <td>DL</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>c-10</td>\n",
       "      <td>c-29</td>\n",
       "      <td>c-1</td>\n",
       "      <td>834</td>\n",
       "      <td>OO</td>\n",
       "      <td>MKE</td>\n",
       "      <td>MSP</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Month DayofMonth DayOfWeek  DepTime UniqueCarrier Origin Dest  Distance\n",
       "0       c-7       c-25       c-3      615            YV    MRY  PHX       598\n",
       "1       c-4       c-17       c-2      739            WN    LAS  HOU      1235\n",
       "2      c-12        c-2       c-7      651            MQ    GSP  ORD       577\n",
       "3       c-3       c-25       c-7     1614            WN    BWI  MHT       377\n",
       "4       c-6        c-6       c-3     1505            UA    ORD  STL       258\n",
       "...     ...        ...       ...      ...           ...    ...  ...       ...\n",
       "99995   c-6        c-5       c-2      852            WN    CRP  HOU       187\n",
       "99996  c-11       c-24       c-6     1446            UA    ORD  LAS      1515\n",
       "99997   c-1       c-30       c-2     1509            OO    ORD  SGF       438\n",
       "99998   c-1        c-5       c-5      804            DL    LGA  ATL       761\n",
       "99999  c-10       c-29       c-1      834            OO    MKE  MSP       297\n",
       "\n",
       "[100000 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cj6bfSNw_RAf"
   },
   "outputs": [],
   "source": [
    "#DROP THE THING WE'RE TRYNA PREDICT! like making the groom leave the room for the bride lollll\n",
    "full_df = pd.concat([train_df.drop('dep_delayed_15min', axis=1), test_df])\n",
    "full_df = feature_eng(full_df)\n",
    "\n",
    "#hmmm okay, speaking of groom, w/ this now we're gonna 'groom' the FULL dataset where only the training portion was treated - .... OHHHHHHH! okay, so no - not really, we never did any 'treating' – we simply EXPLORED the full training data / EDA – well, we ALMOST/WOULD’VE cleaned out the TIME column w/ the >2400! but didn’t lol. so yeah, that was just exploration, and so now we’re gonna COMBINE the test & train df, but *NOTE*, the *TEST DF* ALREADY HAS THE TARGET FEATURE y DROPPED!!!!! SO WE’RE JUST GONNA DROP IT FROM THE TRAINING AND THEN CONCAT THE 2 AND THEY’LL BE ALIGNED!!!!!\n",
    "#again, don’t know why we did like this? maybe just showing us a diff way we might see things in the field? but yeah, now w/ the target y to be predicted gone, we’ll have our X_FEATURES matrix!!! so note, that we’ll apply all these functions we just made to this COMBINED x_features matrix to add columns/feature engineer our combined train+test dataset!!!!!!\n",
    "\n",
    "#NOTE! in y=mx+b Bayesian we were predicting in the sense of that is what we’re AIMING to do with the y=mx+b equation ultimately!!!!! and like when we’re optimizing, we’re picking / testing m/b’s to come up with PREDICTED Y’S!!!!!! that we use for our yp’s in the NormDistEqn!!!!!\n",
    "#Here, we’re tryna predict whether there’s a delay or not based on all the other info. So this is classification rather than regression, and, as we’ll see, will be done via a decision tree ensemble which is what Light Gradient Boosting Machine does. What ‘equation’ is it using? Not sure – but I guess concept is the same – I guess it wouldn't be minimizing SQUARED LOSS/ERROR, but rather the error it’d be minimizing is simply straight ACCURACY error?! Aka MAXIMIZING ACCURACY (or whatev metric most appropriate for the application)?! cuz like we give it the ‘right answers’!!!!! so it can simply grade/score it\n",
    "\n",
    "#Remem! we did save the target y's aka DELAYED15MINS in TRAINING as its own/as y_train, just didn't NIX it off officially!\n",
    "#separate the y's off  - JUST the y's!!!!!! and that makes sense cuz we need the y's for TRAINING but *NOT* to be included in TEST!!! until the END!\n",
    "#that makes sense cuz we need the TRAINING y's to train, and the test y's will be used just to grade/score\n",
    "#final performance, but... don't see the test y's anywhere?! lol we never evaluate this model performance?!?!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSO8JbfM_W-F"
   },
   "source": [
    "Apply the earlier defined feature engineering functions to the full dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6RfAINftjwi"
   },
   "outputs": [],
   "source": [
    "for column in ['UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']:\n",
    "    full_df[column] = label_enc(full_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJAw1RGB_ZuM"
   },
   "source": [
    "\n",
    "Split the new full dataframe into X_train and X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15cPtQU5tjfz"
   },
   "outputs": [],
   "source": [
    "#so... just splitting it BACK to train & test, except this time WITHOUT the y, and now feature_engineered features\n",
    "#just using the SHAPE of original train & test\n",
    "X_train = full_df[:train_df.shape[0]]\n",
    "X_test = full_df[train_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umfAw-9JErLV"
   },
   "source": [
    "Create a list of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:31:58.412296Z",
     "start_time": "2019-04-22T14:31:58.409088Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5ibeVyNb-KZI"
   },
   "outputs": [],
   "source": [
    "categorical_features = ['Month',  'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzMIsMPIETVk"
   },
   "source": [
    "Let's build a light GBM model to test the bayesian optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grape. GradientBoosting was the one thing i didn't really learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:18:04.466965Z",
     "start_time": "2019-04-22T15:18:04.457992Z"
    },
    "colab_type": "text",
    "id": "2hfm1i5G-KZH"
   },
   "source": [
    "### [LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "* Faster training speed and higher efficiency.\n",
    "* Lower memory usage.\n",
    "* Better accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jf-3F2Wg-KZL"
   },
   "source": [
    "First, we define the function we want to maximize and that will count cross-validation metrics of lightGBM for our parameters.\n",
    "\n",
    "Some params such as num_leaves, max_depth, min_child_samples, min_data_in_leaf should be integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay so they're saying like, just like we saw above simple & w/ Bayes NUTS NormDistEqn, we gotta have a\n",
    "#FUNCTION TO MAXIMIZE!!!! so that's what we're doing below, w/ LightGBM, which will utilize decision trees\n",
    "#for this classification prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:40:14.034265Z",
     "start_time": "2019-04-22T15:40:14.027868Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LyUJBhGX-KZM"
   },
   "outputs": [],
   "source": [
    "#dang so we gotta put in ALL THESE?! and NONE OF 'EM HAVE DEFAULTS?!?!\n",
    "########################################################################\n",
    "def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n",
    "    params = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : \"auc\", \n",
    "        'is_unbalance': True,\n",
    "        \"num_leaves\" : int(num_leaves),\n",
    "        \"max_depth\" : int(max_depth),\n",
    "        \"lambda_l2\" : lambda_l2,\n",
    "        \"lambda_l1\" : lambda_l1,\n",
    "        \"num_threads\" : 20,\n",
    "        \"min_child_samples\" : int(min_child_samples),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        \"learning_rate\" : 0.03,\n",
    "        \"subsample_freq\" : 5,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
    "    cv_result = lightgbm.cv(params,\n",
    "                       lgtrain,\n",
    "                       1000,\n",
    "                       early_stopping_rounds=100,\n",
    "                       stratified=True,\n",
    "                       nfold=3)\n",
    "    return cv_result['auc-mean'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJwqBhdeF11Q"
   },
   "source": [
    "Apply the Bayesian optimizer to the function we created in the previous step to identify the best hyperparameters. We will run 10 iterations and set init_points = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:48:04.682447Z",
     "start_time": "2019-04-22T15:40:14.641634Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JheCOkUE-KZP",
    "outputId": "8f37ee51-885d-44e4-cdcd-ceb7abd58b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | lambda_l1 | lambda_l2 | max_depth | min_ch... | min_da... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=444, min_child_samples=6077 will be ignored. Current value: min_data_in_leaf=444\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7193   \u001b[0m | \u001b[0m0.002822 \u001b[0m | \u001b[0m0.03023  \u001b[0m | \u001b[0m49.67    \u001b[0m | \u001b[0m6.078e+03\u001b[0m | \u001b[0m444.6    \u001b[0m | \u001b[0m678.6    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.7194   \u001b[0m | \u001b[95m0.04816  \u001b[0m | \u001b[95m0.01211  \u001b[0m | \u001b[95m9.398    \u001b[0m | \u001b[95m6.357e+03\u001b[0m | \u001b[95m308.9    \u001b[0m | \u001b[95m1.189e+03\u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.7437   \u001b[0m | \u001b[95m0.04154  \u001b[0m | \u001b[95m0.01364  \u001b[0m | \u001b[95m9.394    \u001b[0m | \u001b[95m5.024e+03\u001b[0m | \u001b[95m1.975e+03\u001b[0m | \u001b[95m2.909e+03\u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.7436   \u001b[0m | \u001b[0m0.04682  \u001b[0m | \u001b[0m0.01212  \u001b[0m | \u001b[0m31.43    \u001b[0m | \u001b[0m3.594e+03\u001b[0m | \u001b[0m1.994e+03\u001b[0m | \u001b[0m3.903e+03\u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.728    \u001b[0m | \u001b[0m0.00967  \u001b[0m | \u001b[0m0.01261  \u001b[0m | \u001b[0m60.63    \u001b[0m | \u001b[0m2.466e+03\u001b[0m | \u001b[0m878.2    \u001b[0m | \u001b[0m971.1    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.7437   \u001b[0m | \u001b[95m0.04369  \u001b[0m | \u001b[95m0.04999  \u001b[0m | \u001b[95m52.54    \u001b[0m | \u001b[95m6.162e+03\u001b[0m | \u001b[95m1.961e+03\u001b[0m | \u001b[95m3.999e+03\u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.7436   \u001b[0m | \u001b[0m0.01034  \u001b[0m | \u001b[0m0.005276 \u001b[0m | \u001b[0m40.56    \u001b[0m | \u001b[0m7.236e+03\u001b[0m | \u001b[0m1.479e+03\u001b[0m | \u001b[0m2.311e+03\u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.7205   \u001b[0m | \u001b[0m0.03994  \u001b[0m | \u001b[0m0.01464  \u001b[0m | \u001b[0m60.12    \u001b[0m | \u001b[0m3.655e+03\u001b[0m | \u001b[0m262.2    \u001b[0m | \u001b[0m184.8    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.7325   \u001b[0m | \u001b[0m0.04985  \u001b[0m | \u001b[0m0.02312  \u001b[0m | \u001b[0m15.17    \u001b[0m | \u001b[0m457.0    \u001b[0m | \u001b[0m997.8    \u001b[0m | \u001b[0m67.32    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.7433   \u001b[0m | \u001b[0m0.03447  \u001b[0m | \u001b[0m0.04401  \u001b[0m | \u001b[0m19.32    \u001b[0m | \u001b[0m1.947e+03\u001b[0m | \u001b[0m1.908e+03\u001b[0m | \u001b[0m223.4    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.7307   \u001b[0m | \u001b[0m0.03942  \u001b[0m | \u001b[0m0.02395  \u001b[0m | \u001b[0m57.42    \u001b[0m | \u001b[0m1.495e+03\u001b[0m | \u001b[0m960.1    \u001b[0m | \u001b[0m49.55    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.7304   \u001b[0m | \u001b[0m0.03917  \u001b[0m | \u001b[0m0.01426  \u001b[0m | \u001b[0m9.473    \u001b[0m | \u001b[0m9.8e+03  \u001b[0m | \u001b[0m895.2    \u001b[0m | \u001b[0m3.826e+03\u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#aka TWO CALIBRATION ITERATIONS!!!\n",
    "#so then why don't they call it 'init_iter' or 'init_steps'?!?!? why init 'POINTS'?!?!?!?!?!? that seems really\n",
    "#misleading / confusing no?!?!?!?!?!!? unless i'm understanding it wrong and they're NOT iterations?!\n",
    "\n",
    "#leaves! max_depth! so they were right! this is decision tree stuff!!!\n",
    "#Here's wiki!:\n",
    "#\"Gradient boosting is a machine learning technique used in regression and classification tasks, among others.\n",
    "#It gives a prediction model in the form of an ensemble of weak prediction models, which are typically\n",
    "#decision trees.[1][2] When a decision tree is the weak learner, the resulting algorithm is called\n",
    "#gradient-boosted trees; it usually outperforms random forest.[1][2][3] A gradient-boosted trees model is built\n",
    "#in a stage-wise fashion as in other boosting methods, but it generalizes the other methods by allowing\n",
    "#optimization of an arbitrary differentiable loss function.\" >>>THAT EXPLAINS IT!!!!!!\n",
    "\n",
    "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
    "                                                'max_depth': (5, 63),\n",
    "                                                'lambda_l2': (0.0, 0.05),\n",
    "                                                'lambda_l1': (0.0, 0.05),\n",
    "                                                'min_child_samples': (50, 10000),\n",
    "                                                'min_data_in_leaf': (100, 2000)\n",
    "                                                })\n",
    "\n",
    "#ahhhhhhhhhhhhhhhhh, okay, so, lgb_eval, which we just defined above/made as a function,\n",
    "#is our OPTIMIZER function, and it takes all those arguments, as shown in the cell block above,\n",
    "#and the values for those are GIVEN HERE IN THE {} ARRAY!!!!! remem, NO DEFAULTS SPECIFIED!!!!! SO GOTTA FILL/SPECIFY!!!!!\n",
    "#normally we're used to calling custom functions by just like lgb_eval(num_leaves=(25,400)....etc etc!)....\n",
    "#BUT! REMEM! THIS IS HOW WE USED THE BAYESIANOPTIMIZER FUNCTION CALL IN OUR SIMPLE SAMPLE EXAMPLE ABOVE!!!!!!\n",
    "#aka 'simple_func' was simply a+b:\n",
    "# def simple_func(a, b):\n",
    "#     return a + b\n",
    "\n",
    "#so w/ BO, we did, remem:\n",
    "\n",
    "# optimizer = BayesianOptimization(simple_func,{'a': (1, 3),'b': (4, 7)})\n",
    "\n",
    "#aka we DEFINED/SPECIFIED WHAT THE PARAMETER VALUES a & b WERE!!!!!!!!!!!!!!\n",
    "#########################################################################################################\n",
    "#terminology a little confusing tho cuz lgb_eval() func^^ has that WHOLE LIST of 'params' as it calls them,\n",
    "#but then what we'd normally call 'args' of the functions are the hyperparams of this function we're tryna optimize!!!!!!\n",
    "#remem - params are not variables but *CONSTANTS*!!!!! like in y=mx+b, m&b: constants/params, y/x: VARIABLES!!!!!!\n",
    "#########################################################################################################\n",
    "\n",
    "lgbBO.maximize(n_iter=10, init_points=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OHHHHHHH, got it - so just like we had columns of target | a | b in our simple sample, HERE WE HAVE\n",
    "#COLUMNS FOR THE VALUES OF *EACH OF OUR HYPERPARAMETERS!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#AGAIN, THESE HYPERPARAMS ARE SIMPLY THE *ARGS* OF OUR CUSTOM DEFINED LGB_EVAL() FUNCTION!!!!!!!\n",
    "#AND THE \"TARGET\" IS THE F(X) VALUE OF OUR OPTIMIZER FUNCTION!!!!!! AKA REMEM WE'RE TRYNA GET THE *MAXIMUM*\n",
    "#OF THAT!!!!!! GIVEN THE COMBINATION OF HYPERPARAM VALUES!!!!!\n",
    "#SO LIKE IN BEST FIT LINE / NORMALDISTEQN NUTS, THE 'TARGET' WAS THE F(X) OF THE NORMALDISTEQN WE WERE TRYNA\n",
    "#GET MAX OF WHICH REPRESENTED *MINIMUM SQUARED ERROR*!!!!!!\n",
    "#SO IT'S GONNA BE ONE OF THE PINK ROWS OFC!!!!!\n",
    "#HERE INTERESTINGLY IT'S A TIE FOR AS FAR AS DECIMALS WE CAN SEE!!! 2nd & 6th iter!!!\n",
    "#so would have to look closer or jsut pick one lol\n",
    "\n",
    "#BUT AGAIN - WHAT IS THE ACTUAL LIKE \"EQUATION\" WE'RE USING/OPTIMIZING?! LIKE THAT WE COULD PLOT?!\n",
    "#AND ALSO, *WHERE IS THE ***y_TEST*** FOR JUDGING THE BEST PERFORMER?!\n",
    "#OR ARE WE *ONLY* DOING IT ON TRAIN??\n",
    "#BUT IF WE'RE ONLY DOING ON TRAIN, WHAT WAS THE POINT OF SPLITTING INTO TEST?!\n",
    "#OR IS THIS JUST LIKE GETTING US STARTED?! and leaving out the rest for simplicity?!\n",
    "#but yeah, WONDERING LIKE WHAT DO THESE TARGET VALUES REPRESENT?! LIKE IS IT *ACCURACY*?!? which would be cool cuz,\n",
    "#unlike squared error, here we *ALREADY* WANT THE MAX!!!!! THE HIGHER THE BETTER!!!!!\n",
    "\n",
    "\n",
    "#OHHHHHHHHHHHHHHHHH! okay just saw / looked closer now! So - YES - we ARE only doing on TRAIN! look at this piece\n",
    "#from above - the BOTTOM piece/SECOND half! >> 'lgTRAIN'!!!!!!:\n",
    "\n",
    "    # lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
    "    # cv_result = lightgbm.cv(params,\n",
    "    #                    lgtrain,\n",
    "    #                    1000,\n",
    "    #                    early_stopping_rounds=100,\n",
    "    #                    stratified=True,\n",
    "    #                    nfold=3)\n",
    "    # return cv_result['auc-mean'][-1]\n",
    "\n",
    "#so yeah lgtrain IS just for TRAIN, and as you see the next line is the *CROSS-VALIDATION RESULT* which\n",
    "#USES LGTRAIN!!!!!!! AND THEN THE VERY LAST LINE SHOWS US THE SCORING METRIC WHICH IS NONE OTHER THAN\n",
    "# 'AUC'!!!!!!!!!!!! the mean to be exact!\n",
    "# which is indeed something we can use in Classification!!! which combines diff of the other classification/confusion\n",
    "#metrics!!!!!! quantifies like trade-off!\n",
    "####################################################################################################################\n",
    "\n",
    "#Okay, but - if we did just train, and we're doing DECISION trees esp, THEN WOULDN'T/SHOULDN'T THE\n",
    "#MAX HAVE BEEN A *PERFECT* OVERFIT SCORE OF 1!?!??!?!?!??!?!!?!?!??!!?!?!??!!?!?!?\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohhh, so if you watch it in real-time come up w/ this list / go thru these iterations,\n",
    "#it makes pink/purple the CURRENT max/best up till then?? but then it shoulda made the VERY FIRST purp then\n",
    "#no? maybe it excludes that. AHHHH, yeah, so i think that's right! that adds up! is consistent with what we see!\n",
    "#cuz if you look at the TARGET column, which is what we're after the max of, like back in our run\n",
    "#in the beginning when we were confused on why there were 2 purple rows, it's cuz TWICE the current run's target\n",
    "#was the NEW HIGH! and yeah, the very last one ended up being the HIGHEST and MAX HIGH POSSIBLE?!\n",
    "#and if you look at their pre-loaded version, the 4th was the overall max, and it worked out that yeah - that's\n",
    "#correct, NO OTHER ROW SHOULD'VE LIT UP/BEEN HIGHLIGHTED CUZ NO ONE EVER DETHRONED THE CURRENT MAX (lol like the name!\n",
    "# like, who's the MAX?!)/ KING'S COURT! 7.8, 7.1, 7.4 [10.0], 9.4!!!!!!!\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SO WHAT WAS THAT WHOLE *GIF* THING ABOUT RELATIVE TO THIS?!? HOW DOES THAT APPLY HERE / WHAT DOES IT LOOK\n",
    "#LIKE FOR HERE?! maybe gotta look at full documentation example - took a quick look n didn't see, nor about\n",
    "#yTEST!!!!!!\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what's the warning about?:\n",
    "#[Warning] min_data_in_leaf is set=444, min_child_samples=6077 will be ignored.\n",
    "#Current value: min_data_in_leaf=444\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdkxhhST-KZS"
   },
   "source": [
    " **<font color='teal'> Print the best result by using the '.max' function.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:49:01.513767Z",
     "start_time": "2019-04-22T15:49:01.509392Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oc8z6mfy-KZS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.743677184606366,\n",
       " 'params': {'lambda_l1': 0.04368878406260319,\n",
       "  'lambda_l2': 0.04998807104235295,\n",
       "  'max_depth': 52.535726432546326,\n",
       "  'min_child_samples': 6161.725956471824,\n",
       "  'min_data_in_leaf': 1960.867045207072,\n",
       "  'num_leaves': 3998.716330891756}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:50:29.049881Z",
     "start_time": "2019-04-22T15:50:29.045908Z"
    },
    "colab_type": "text",
    "id": "J5LAydKC-KZW"
   },
   "source": [
    "Review the process at each step by using the '.res[0]' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:51:01.001688Z",
     "start_time": "2019-04-22T15:51:00.997484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "X1ttZmrI-KZX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.7192540779609389,\n",
       " 'params': {'lambda_l1': 0.002822101387856496,\n",
       "  'lambda_l2': 0.030229580059482154,\n",
       "  'max_depth': 49.66714825748048,\n",
       "  'min_child_samples': 6077.71394513601,\n",
       "  'min_data_in_leaf': 444.62104547614024,\n",
       "  'num_leaves': 678.6063438891952}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohh okay, so lgbBO ('light gradient boosting bayesian optimizer'?) MAX is the MAX target run/RES,\n",
    "#aka [RES] is AN INDIVIDUAL RUN! so this one, [0] is simply the first one above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh WOOWWWWWWWW! wait - you gotta be kidding me - we did / commented on ALLLLLLLLLLLLL those feature engineering\n",
    "#added columns and functions for treating and DIDN'T USE ANY OF THEM EXCEPT THE VERY FIRST ONE / LABEL ENCODER?!?!?!??!!?!?\n",
    "#####################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bayesian_optimization_exercise.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4047717ba1451957ab119556882dc66ed44e0c89a4f9c3013aa6cec5da1740d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
